{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Getting Started with OpenAI API\n",
    "## Setting Up the Enviroments\n",
    "Install and Import Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as lt\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tenacity import retry, wait_exponential\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"your key here\" # Please refer to assignment for obtaining the key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: SQuADv2 Data Subsets\n",
    "As this assignment is designed for getting familiar with GPT usage and fine-tuning, small slices from the train and validation splits of the original dataset is enough. We provide the function for processing the dataset, you only need to load the dataset and call the functions.\n",
    "\n",
    "Please read the data from the JSON files and create a adataframe with the following columns: `question`, `context`, `answer`, `is_impossible`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original train and dev data, make sure the path is correct\n",
    "with open(r\"your path to the train-v2.0.json file\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train = json.load(f)\n",
    "    \n",
    "with open(r\"your path to the dev-v2.0.json file\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dev = json.load(f)\n",
    "    \n",
    "def json_to_dataframe_with_titles(json_data):\n",
    "    qas = []\n",
    "    context = []\n",
    "    is_impossible = []\n",
    "    answers = []\n",
    "    titles = []\n",
    "\n",
    "    for article in json_data['data']:\n",
    "        title = article['title']\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                qas.append(qa['question'].strip())\n",
    "                context.append(paragraph['context'])\n",
    "                is_impossible.append(qa['is_impossible'])\n",
    "                \n",
    "                ans_list = []\n",
    "                for ans in qa['answers']:\n",
    "                    ans_list.append(ans['text'])\n",
    "                answers.append(ans_list)\n",
    "                titles.append(title)\n",
    "\n",
    "    df = pd.DataFrame({'title': titles, 'question': qas, 'context': context, 'is_impossible': is_impossible, 'answers': answers})\n",
    "    return df\n",
    "\n",
    "def get_diverse_sample(df, sample_size=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Get a diverse sample of the dataframe by sampling from each title\n",
    "    \"\"\"\n",
    "    sample_df = df.groupby(['title', 'is_impossible']).apply(lambda x: x.sample(min(len(x), max(1, sample_size // 50)), random_state=random_state)).reset_index(drop=True)\n",
    "    \n",
    "    if len(sample_df) < sample_size:\n",
    "        remaining_sample_size = sample_size - len(sample_df)\n",
    "        remaining_df = df.drop(sample_df.index).sample(remaining_sample_size, random_state=random_state)\n",
    "        sample_df = pd.concat([sample_df, remaining_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return sample_df.sample(min(sample_size, len(sample_df)), random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# obtaining dataframe for further message format process\n",
    "train_sample = json_to_dataframe_with_titles(train)\n",
    "dev_sample = json_to_dataframe_with_titles(dev)\n",
    "train_df = get_diverse_sample(train_sample, sample_size=300)\n",
    "dev_df = get_diverse_sample(dev_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Implement Basic API Calls\n",
    "In this part, you need to get familiar with the message format by OpenAI API and make your API calls with the provided functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create msg according to the API format\n",
    "def create_msg(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "    Question: {row.question}\\n\\n\n",
    "    Context: {row.context}\\n\\n\n",
    "    Answer:\\n\"\"\",\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# sometimes the call would fail, we can retry with tenacity\n",
    "@retry(wait = wait_exponential(multipler=1, min=2, max=6))\n",
    "def api_call(msgs, model):\n",
    "    # call pai for returning the responses\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stop=[\"\\n\\n\"],\n",
    "        max_tokens=100,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "def answer_question(row, model=\"gpt-3.5-turbo-1106\"):\n",
    "    # feel free to use other models, slections can be seen here: https://platform.openai.com/docs/guides/fine-tuning\n",
    "    msg = create_msg(row)\n",
    "    response = api_call(msg, model)\n",
    "    return response[\"choice\"][0][\"message\"][\"content\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Your Code Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make your request using the function provided above\n",
    "test_row = train_df.iloc[0]\n",
    "response = \n",
    "# print the content of response out\n",
    "print()\n",
    "\n",
    "# Obtain all answers from vanilla model\n",
    "dev_df[\"generated_answer\"] = df.progress_apply(answer_question, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fine-Tune GPT on SQuADv2 Dataset\n",
    "Get our key, data prepared then we're ready to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare fine-tuning data, convert data into messages format\n",
    "def dataframe_to_jsonl(df):\n",
    "    def create_jsonl_entry(row):\n",
    "        answer = row[\"answers\"][0] if row[\"answers\"] else \"I don't know\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Answer the following Question based on the Context only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\n",
    "            Question: {row.question}\\n\\n\n",
    "            Context: {row.context}\\n\\n\n",
    "            Answer:\\n\"\"\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        return json.dumps({\"messages\": messages})\n",
    "\n",
    "    jsonl_output = df.apply(create_jsonl_entry, axis=1)\n",
    "    return \"\\n\".join(jsonl_output)\n",
    "\n",
    "# the dumped file will be upload towards fine-tuning\n",
    "with open(r\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dataframe_to_jsonl(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIFineTuner:\n",
    "    \"\"\"\n",
    "    Class to fine tune OpenAI models\n",
    "    \"\"\"\n",
    "    def __init__(self, training_file_path, model_name, suffix):\n",
    "        self.training_file_path = training_file_path\n",
    "        self.model_name = model_name\n",
    "        self.suffix = suffix\n",
    "        self.file_object = None\n",
    "        self.fine_tuning_job = None\n",
    "        self.model_id = None\n",
    "\n",
    "    def create_openai_file(self):\n",
    "        self.file_object = openai.File.create(\n",
    "            file=open(self.training_file_path, \"r\"),\n",
    "            purpose=\"fine-tune\",\n",
    "        )\n",
    "\n",
    "    def wait_for_file_processing(self, sleep_time=20):\n",
    "        while self.file_object.status != 'processed':\n",
    "            time.sleep(sleep_time)\n",
    "            self.file_object.refresh()\n",
    "            print(\"File Status: \", self.file_object.status)\n",
    "\n",
    "    def create_fine_tuning_job(self):\n",
    "        self.fine_tuning_job = openai.FineTuningJob.create(\n",
    "            training_file=self.file_object[\"id\"],\n",
    "            model=self.model_name,\n",
    "            suffix=self.suffix,\n",
    "        )\n",
    "\n",
    "    def wait_for_fine_tuning(self, sleep_time=45):\n",
    "        while self.fine_tuning_job.status != 'succeeded':\n",
    "            time.sleep(sleep_time)\n",
    "            self.fine_tuning_job.refresh()\n",
    "            print(\"Job Status: \", self.fine_tuning_job.status)\n",
    "\n",
    "    def retrieve_fine_tuned_model(self):\n",
    "        self.model_id = openai.FineTuningJob.retrieve(self.fine_tuning_job[\"id\"]).fine_tuned_model\n",
    "        return self.model_id\n",
    "\n",
    "    def fine_tune_model(self):\n",
    "        self.create_openai_file()\n",
    "        self.wait_for_file_processing()\n",
    "        self.create_fine_tuning_job()\n",
    "        self.wait_for_fine_tuning()\n",
    "        return self.retrieve_fine_tuned_model()\n",
    "\n",
    "fine_tuner = OpenAIFineTuner(\n",
    "        # put your dumped train.jsonl file here\n",
    "        training_file_path=\"path to your train.jsonl file\",\n",
    "        model_name=\"gpt-3.5-turbo-1106\",\n",
    "        suffix=\"cs4341gpt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print your fine-tuned model\n",
    "model_id = fine_tuner.fine_tune_model()\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Your Code Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your fine-tuned model out\n",
    "completeion = openai.ChatCompletion.create(\n",
    "    model=model_id,\n",
    "    # according to the previous message format, design your own question here\n",
    "    messages=[\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Questions for Evaluation\n",
    "**Write Your Code Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all answers from fine-tuned model \n",
    "dev_df[\"ft_generated_answer\"] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Evaluation & Comparison**\n",
    "In this part, we will evaluate the fine-tuned model in terms of the generated answers and comapre the difference between the fine-tuned model and the vanilla model. As detailed in the Assignment, we measure the model's performance using the following schema:\n",
    "\n",
    "1. ✅ **Answered Correctly**: The model responded the correct answer. It may have also included other answers that were not in the context.\n",
    "2. ❎ **Skipped**: The model responded with \"I don't know\" (IDK) while the answer was present in the context. It's better than giving the wrong answer. It's better for the model say \"I don't know\" than giving the wrong answer. In our design, we know that a true answer exists and hence we're able to measure it -- this is not always the case. *This is a model error*. We exclude this from the overall error rate. \n",
    "3. ❌ **Wrong**: The model responded with an incorrect answer. **This is a model ERROR.**\n",
    "\n",
    "When we know that a correct answer does not exist in the context, we can measure the model's performance, there are 2 possible outcomes:\n",
    "\n",
    "4. ❌ **Hallucination**: The model responded with an answer, when \"I don't know\" was expected. **This is a model ERROR.** \n",
    "5. ✅ **I don't know**: The model responded with \"I don't know\" (IDK) and the answer was not present in the context. **This is a model WIN.**\n",
    "\n",
    "Finally, we plot the results generated by both fine-tuned model and the vanilla model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.y_pred = pd.Series()  # Initialize as empty Series\n",
    "        self.labels_answer_expected = [\"✅ Answered Correctly\", \"❎ Skipped\", \"❌ Wrong Answer\"]\n",
    "        self.labels_idk_expected = [\"❌ Hallucination\", \"✅ I don't know\"]\n",
    "\n",
    "    def _evaluate_answer_expected(self, row, answers_column):\n",
    "        generated_answer = row[answers_column].lower()\n",
    "        actual_answers = [ans.lower() for ans in row[\"answers\"]]\n",
    "        return (\n",
    "            \"✅ Answered Correctly\" if any(ans in generated_answer for ans in actual_answers)\n",
    "            else \"❎ Skipped\" if generated_answer == \"i don't know\"\n",
    "            else \"❌ Wrong Answer\"\n",
    "        )\n",
    "\n",
    "    def _evaluate_idk_expected(self, row, answers_column):\n",
    "        generated_answer = row[answers_column].lower()\n",
    "        return (\n",
    "            \"❌ Hallucination\" if generated_answer != \"i don't know\"\n",
    "            else \"✅ I don't know\"\n",
    "        )\n",
    "\n",
    "    def _evaluate_single_row(self, row, answers_column):\n",
    "        is_impossible = row[\"is_impossible\"]\n",
    "        return (\n",
    "            self._evaluate_answer_expected(row, answers_column) if not is_impossible\n",
    "            else self._evaluate_idk_expected(row, answers_column)\n",
    "        )\n",
    "\n",
    "    def evaluate_model(self, answers_column=\"generated_answer\"):\n",
    "        self.y_pred = pd.Series(self.df.apply(self._evaluate_single_row, answers_column=answers_column, axis=1))\n",
    "        freq_series = self.y_pred.value_counts()\n",
    "        \n",
    "        # Counting rows for each scenario\n",
    "        total_answer_expected = len(self.df[self.df['is_impossible'] == False])\n",
    "        total_idk_expected = len(self.df[self.df['is_impossible'] == True])\n",
    "        \n",
    "        freq_answer_expected = (freq_series / total_answer_expected * 100).round(2).reindex(self.labels_answer_expected, fill_value=0)\n",
    "        freq_idk_expected = (freq_series / total_idk_expected * 100).round(2).reindex(self.labels_idk_expected, fill_value=0)\n",
    "        return freq_answer_expected.to_dict(), freq_idk_expected.to_dict()\n",
    "\n",
    "    def print_eval(self):\n",
    "        answer_columns=[\"generated_answer\", \"ft_generated_answer\"]\n",
    "        baseline_correctness, baseline_idk = self.evaluate_model()\n",
    "        ft_correctness, ft_idk = self.evaluate_model(self.df, answer_columns[1])\n",
    "        print(\"When the model should answer correctly:\")\n",
    "        eval_df = pd.merge(\n",
    "            baseline_correctness.rename(\"Baseline\"),\n",
    "            ft_correctness.rename(\"Fine-Tuned\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "        print(eval_df)\n",
    "        print(\"\\n\\n\\nWhen the model should say 'I don't know':\")\n",
    "        eval_df = pd.merge(\n",
    "            baseline_idk.rename(\"Baseline\"),\n",
    "            ft_idk.rename(\"Fine-Tuned\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "        print(eval_df)\n",
    "    \n",
    "    def plot_model_comparison(self, answer_columns=[\"generated_answer\", \"ft_generated_answer\"], scenario=\"answer_expected\", nice_names=[\"Baseline\", \"Fine-Tuned\"]):\n",
    "        \n",
    "        results = []\n",
    "        for col in answer_columns:\n",
    "            answer_expected, idk_expected = self.evaluate_model(col)\n",
    "            if scenario == \"answer_expected\":\n",
    "                results.append(answer_expected)\n",
    "            elif scenario == \"idk_expected\":\n",
    "                results.append(idk_expected)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid scenario\")\n",
    "        \n",
    "        \n",
    "        results_df = pd.DataFrame(results, index=nice_names)\n",
    "        if scenario == \"answer_expected\":\n",
    "            results_df = results_df.reindex(self.labels_answer_expected, axis=1)\n",
    "        elif scenario == \"idk_expected\":\n",
    "            results_df = results_df.reindex(self.labels_idk_expected, axis=1)\n",
    "        \n",
    "        melted_df = results_df.reset_index().melt(id_vars='index', var_name='Status', value_name='Frequency')\n",
    "        sns.set_theme(style=\"whitegrid\", palette=\"icefire\")\n",
    "        g = sns.catplot(data=melted_df, x='Frequency', y='index', hue='Status', kind='bar', height=5, aspect=2)\n",
    "\n",
    "        # Annotating each bar\n",
    "        for p in g.ax.patches:\n",
    "            g.ax.annotate(f\"{p.get_width():.0f}%\", (p.get_width()+5, p.get_y() + p.get_height() / 2),\n",
    "                        textcoords=\"offset points\",\n",
    "                        xytext=(0, 0),\n",
    "                        ha='center', va='center')\n",
    "        plt.ylabel(\"Model\")\n",
    "        plt.xlabel(\"Percentage\")\n",
    "        plt.xlim(0, 100)\n",
    "        plt.tight_layout()\n",
    "        plt.title(scenario.replace(\"_\", \" \").title())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Compare the results\n",
    "evaluator = Evaluator(dev_df)\n",
    "evaluator.evaluate_model(answers_column=\"ft_generated_answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Your Code Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model comparison in the scenario of \"answer_expected\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model comparison in the scenario of \"answer_expected\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Observations\n",
    "## Please write your findings through the whole process here\n",
    "This is an open question, feel free to write your takeways through the whole assignment as well as the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
